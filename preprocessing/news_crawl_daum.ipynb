{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPOmMA5no7t24y3ONuodfJx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 필요 패키지 설치"],"metadata":{"id":"rn-JBe7R8ikH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3AF47VOw8SX0"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup"]},{"cell_type":"markdown","source":["# 실행 코드"],"metadata":{"id":"_oWemzjP9cxC"}},{"cell_type":"code","source":["def crawl_daum_news(url):\n","    \"\"\"\n","    주어진 다음 뉴스 URL에서 제목과 본문 내용을 추출\n","\n","    Args:\n","        url (str): Daum 뉴스 기사 URL\n","\n","    Returns:\n","        tuple: (제목, 본문 문자열)\n","    \"\"\"\n","    try:\n","        res = requests.get(url)\n","        res.raise_for_status()\n","    except requests.exceptions.RequestException as e:\n","        print(f\"[에러] 요청 실패: {e}\")\n","        return None, None\n","\n","    soup = BeautifulSoup(res.text, \"html.parser\")\n","\n","    # 제목 추출\n","    title_tag = soup.select_one(\"h3.tit_view\")\n","    title = title_tag.text.strip() if title_tag else \"제목 없음\"\n","\n","    # 본문 추출 (본문 p 태그들 합치기)\n","    content_paragraphs = soup.select(\"section p\")\n","    content = \"\\n\".join([p.text.strip() for p in content_paragraphs if p.text.strip()]) or \"본문 없음\"\n","\n","    return title, content\n","\n","if __name__ == \"__main__\":\n","    # 테스트용 Daum 뉴스 URL (실제 뉴스 링크로 교체 필요)\n","    url = \"https://v.daum.net/v/202507050123\"\n","\n","    title, content = crawl_daum_news(url)\n","\n","    print(\"\\n[제목]\")\n","    print(title)\n","    print(\"\\n[본문]\")\n","    print(content)"],"metadata":{"id":"rkxG9B3F9eQM"},"execution_count":null,"outputs":[]}]}